---
title: 'Titanic - ML training'
author: 'Kamil Ch≈Çosta'
date: '12 August 2021'
output:
  html_document:
    number_sections: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: readable
    highlight: tango
---

# Introduction
```{r, include=FALSE}
tree_func <- function(final_model, 
                      tree_num) {
  
  # get tree by index
  tree <- randomForest::getTree(final_model, 
                                k = tree_num, 
                                labelVar = TRUE) %>%
    tibble::rownames_to_column() %>%
    # make leaf split points to NA, so the 0s won't get plotted
    mutate(`split point` = ifelse(is.na(prediction), `split point`, NA))
  
  # prepare data frame for graph
  graph_frame <- data.frame(from = rep(tree$rowname, 2),
                            to = c(tree$`left daughter`, tree$`right daughter`))
  
  # convert to graph and delete the last node that we don't want to plot
  graph <- graph_from_data_frame(graph_frame) %>%
    delete_vertices("0")
  
  # set node labels
  V(graph)$node_label <- gsub("_", " ", as.character(tree$`split var`))
  V(graph)$leaf_label <- as.character(tree$prediction)
  V(graph)$split <- as.character(round(tree$`split point`, digits = 2))
  
  # plot
  plot <- ggraph(graph, 'dendrogram') + 
    theme_bw() +
    geom_edge_link() +
    geom_node_point() +
    geom_node_text(aes(label = node_label), na.rm = TRUE, repel = TRUE) +
    geom_node_label(aes(label = split), vjust = 2.5, na.rm = TRUE, fill = "white") +
    geom_node_label(aes(label = leaf_label, fill = leaf_label), na.rm = TRUE, 
					repel = TRUE, colour = "white", fontface = "bold", show.legend = FALSE) +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major = element_blank(),
          panel.background = element_blank(),
          plot.background = element_rect(fill = "white"),
          panel.border = element_blank(),
          axis.line = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          plot.title = element_text(size = 18))
  
  print(plot)
}

setwd('D:/DataScience/Kaggle/Titanic/Titanic_randomForest_ML_training_Rmarkdown')
```

## Libraries, loading and checking data
```{r setup, message=FALSE, warning=FALSE}
library(caret)
library(ggthemes)
library(scales)
library(mice)
library(stringr)
library(randomForest)
library(ggraph)
library(igraph)
library(ggplot2)
library(dplyr)
library(e1071)
library(vtreat)
library(xgboost)

train <- read.csv("Data/train.csv")
test <- read.csv("Data/test.csv")

tt <- bind_rows(train, test)
```

```{r codeoptions}
# Disable scientific notation in R
options(scipen = 999)

# Set the graphical theme in ggplot2 library
ggplot2::theme_set(ggplot2::theme_light())

# Set global knitr chunk options - avoids cluttered unnecessary info about warnings/messages
knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE
)
```

## First data insights and data wrangling
Checking up data types and getting basic statistical info
```{r firstlook, echo=FALSE}
tt %>% str()
tt %>% summary()
```

Changing some numerical data to categorical
```{r datatype}
tt$Sex <- as.factor(tt$Sex)
tt$Pclass <- as.factor(tt$Pclass)
tt$Embarked <- as.factor(tt$Embarked)
tt$Survived <- as.factor(tt$Survived)
```

## Visualizing the data
Insight:
  There are significantly more males present onboard
  Kids and females are favoured
  There is one old person that survived - consider getting rid of it to avoid higher age bias.
  There is higher probability that the people from class 1 and 2 survived - age has an impact but seems neglectable

```{r FirstGraphs, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(tt %>% tidyr::drop_na(Survived), aes(x = Sex, fill = Survived))+
  geom_bar(aes(y = ..count..))+
  geom_text(stat='count', aes(label=..count..), vjust=-0.5, alpha = 0.5)+
  labs(subtitle = "Survival by sex")

ggplot(tt, aes(x=Age, fill = Sex)) +
  geom_density(alpha = 0.5)+
  labs(subtitle = "Age distribution by sex")

ggplot(tt %>% tidyr::drop_na(Survived) %>% filter(Sex == "male"), aes(x=Age, fill = Survived)) +
  geom_histogram(alpha = 0.4, binwidth = 2)+
  scale_fill_brewer(palette="Dark2")+
  labs(subtitle = "Male survival by age")

ggplot(tt %>% tidyr::drop_na(Survived) %>% filter(Sex == "female"), aes(x=Age, fill = Survived)) +
  geom_histogram(alpha = 0.4, binwidth = 2)+
  scale_fill_brewer(palette="Dark2")+
  labs(subtitle = "Female survival by age")

ggplot(tt %>% tidyr::drop_na(Survived), aes(x=Age, fill = Survived)) +
  geom_histogram(alpha = 0.4, binwidth = 2)+
  scale_fill_brewer(palette="Dark2")+
  labs(subtitle = "Survival by age")

ggplot(tt %>% tidyr::drop_na(Survived), aes(x = Pclass, fill = Survived))+
  geom_bar(aes(y = ..count..))+
  geom_text(stat='count', aes(label=..count..), vjust=-0.5, alpha = 0.5)+
  labs(subtitle = "Survival by Pclass")

ggplot(tt %>% tidyr::drop_na(Survived), aes(x=Age, fill = Survived)) +
  facet_wrap(~Pclass)+
  geom_histogram(alpha = 0.4, binwidth = 2)+
  scale_fill_brewer(palette="Dark2")+
  labs(subtitle = "Survival by age and class")
```

# Feature engineering Part 1/2
## Extracting the data from Name variable

It appears that Name data also contains some potentially usefull info. 
So do SibSp (no of siblings/spouses) and Parch (no of parents/children)
  Creating new variables and insight:
    Title - there are 14 different titles - some are duplicates Mlle, Ms= Miss and Mme = Mrs I guess
    Surname - there are 667 different families - maybe its better to use Family Size
    Family Size - clearly families of 2,3 or 4 are favored to survive
      So lets divide it to a categories:
      1 - One
      2-4 - small
      over 4 - Large

```{r NamesTitles, echo=FALSE, message=FALSE, warning=FALSE}
tt$Title <- gsub('(.*, )|(\\..*)', '', tt$Name)
table(tt$Sex, tt$Title)

# Replacing the same titles but needlessly called different
tt <- tt %>% 
  mutate(Title = case_when(Title == 'Mlle' ~ "Miss",
                           Title == 'Ms' ~ "Miss",
                           Title == 'Mme' ~ "Mrs",
                           TRUE ~ Title),
         FamilySize = Parch+SibSp+1)

ggplot(tt %>% tidyr::drop_na(Survived), aes(x = FamilySize, fill = Survived)) +
  geom_bar(stat='count', position='dodge') +
  labs(x = "Survival by Title") +
  theme_minimal()

ggplot(tt %>% tidyr::drop_na(Survived), aes(x = factor(FamilySize), fill = Survived)) +
  geom_bar(stat='count', position='dodge') +
  labs(x = "Survival by Family Size - cathegorical") +
  theme_minimal()

# Very little occurence of some titles -> calling them "other"
other_titles <- c("Capt", "Col", "Don", "Jonkheer", "Lady", "Major", "Sir", "the Countess", "Dona", "Rev", "Dr")

tt <- tt %>% 
  mutate(Title2 = case_when(Title %in% other_titles ~ "Other",
                           TRUE ~ Title)) %>% 
  mutate(FamilySizeCat = case_when(FamilySize == 1 ~ "One",
                                FamilySize>1 & FamilySize<=4 ~ "Small",
                                FamilySize > 4 ~ "Large"))

ggplot(tt %>% tidyr::drop_na(Survived), aes(x = factor(FamilySizeCat), fill = Survived)) +
  geom_bar(stat='count', position='dodge') +
  labs(x = "Survival by Family Size - cathegorical") +
  theme_minimal()

ggplot(tt %>% tidyr::drop_na(Survived), aes(x = factor(Title), fill = Survived)) +
  geom_bar(stat='count', position='dodge') +
  labs(x = "Survival by Title v2") +
  theme_minimal()

tt$Surname <- as.factor(gsub(",.*", "", tt$Name))
```

## Location - cabin variable

Location onboard - Cabin variable
There is a lot of missing info for most people
Moreover, some people have more than one cabin assignet to their name - usually the numbers are really close so I will just take the first one or skip the numbers and just take the deck? letter and see if there is any correlation with survival rate.
```{r CabinDeckNo}
# Extracting the Deck letter from Cabin variable
tt$Deck <- str_match(tt$Cabin, "[A-Z]")
```

From the future: I saw that this variable seems really significant. Lets see if improving it can influence the results.
Checking if info about the cabin is presents has an impact on survival rate - from the experience with some OHSAS systems, knowledge location of people in danger is a significant factor, lets see if this is true:
```{r cabinv2}
tt <- tt %>% 
  mutate(CabinV2 =  factor(case_when(Cabin != "" | is.na(Cabin) ~ "Specified",
                                     TRUE ~ "Not specified")))

ggplot(tt %>% tidyr::drop_na(Survived), aes(x = CabinV2, fill = Survived))+
  geom_bar(stat='count', position='dodge')+
  geom_text(stat='count', aes(label=..count..), alpha = 0.9)+
  labs(subtitle = "Survival by Cabin Info")
```

This could be significant when iterating with the structure of RF model.

# Couping with missing data 

## Embarked variable
Starting with embark variable, witch is basically the location they started the cruise.
Lets see what's missing from Embarked:

```{r }
a <- tt %>% 
  filter(is.na(Embarked) | Embarked == "")
```
So just two people with PassengerId values 62 and 830 - both are from class 1, both were adults and both paid 80$ for their tickets.
Firstly I have to check if the ticket price varies depending on age to know whether to filter the data by age before guessing the Embark location.
```{r message=FALSE, warning=FALSE}
ggplot(tt, aes(x = Age, y = Fare)) +
  geom_point()+
  geom_smooth()+
  facet_wrap(~Pclass)+
  labs(x = "Does age influences the price of the tickets for given class?")
```

The graph results seems counter-intuitive indication higher prices for younger passengers especially for Pclass = 1.
Out of curiosity: Lets see if sex influenced the ticket price.
```{r sex_ticket_price, echo=TRUE, paged.print=FALSE}
tt %>% group_by(Sex, Pclass) %>% 
  summarise(median(Fare, na.rm=T)) # lol it did. shame on them! but we still need the embark location!

# Lets just see the prices Pclass and Embark location for females
tt %>% 
  filter(Sex == "female") %>% 
  group_by(Pclass, Embarked) %>% 
  summarise(Fare_median = median(Fare, na.rm=T),
            Fare_mean = mean(Fare),
            Amount = n())

# Lets dig deeper - Family size vs price ticket - 
tt %>% 
  filter(Sex == "female", Pclass == "1") %>% 
  group_by(FamilySizeCat, Embarked) %>% 
  summarise(Fare_median = median(Fare, na.rm=T),
            Fare_mean = mean(Fare),
            Amount = n()) %>% data.frame()
```

There is clear difference in ticket price between the classes.
Without further investigation we can assume that the two women Embarked from port "C", even though "S" is also a probable option, but its inferior fit.
```{r EmbarkOverride}
tt$Embarked[c(62, 830)] <- "C"
```

## Fare variable
Lets see other missing data. One person ID 1044 does not have Fare price value:
```{r FareMissing, echo=TRUE, paged.print=FALSE}
tt %>% 
  filter(is.na(Fare) | Fare == "")
```

Lets fit a fare cost based on Embark location, Pclass and sex
```{r FareOverride, echo=TRUE, paged.print=FALSE}
ggplot(tt %>% 
         tidyr::drop_na(Survived) %>% 
         filter(Pclass == 3 & Embarked == "S" & Sex == "male"), 
       aes(x = Fare)) +
  geom_density(fill = 'steelblue', alpha=0.4) + 
  geom_vline(aes(xintercept=median(Fare, na.rm=T)),
    colour='darkred', linetype='dashed', alpha = 0.7) +
  scale_x_continuous(labels=dollar_format())

# Just filling the missing data with median calculated according to filters below:
tt$Fare[1044] <- tt %>% 
  filter(Pclass == 3 & Embarked == "S" & Sex == "male") %>%
  select(Fare) %>%
  summarise(median(Fare, na.rm = T)) %>%
  as.numeric()
```

Make cathegorical variables into factors
```{r CatToFactor2}
tt$PassengerId <- as.factor(tt$PassengerId)
tt$Title <- as.factor(tt$Title)
tt$Title2 <- as.factor(tt$Title2)
tt$FamilySize <- as.factor(tt$FamilySize)
tt$FamilySizeCat <- as.factor(tt$FamilySizeCat)
tt$Deck <- as.factor(tt$Deck)
tt$Survived <- as.factor(tt$Survived)
```

## Age/Deck - Predictive imputation with mice

```{r MiceModelParam, message=FALSE, warning=FALSE}
# Set a random seed
set.seed(111)
# Perform mice imputation, excluding certain less-than-useful variables:
mice_mod <- mice(tt[, !names(tt) %in% c('PassengerId','Name','Ticket','Cabin','FamilySize','Surname','Survived')], method='rf')
```
Save the complete output:
```{r MiceOut}
mice_output <- complete(mice_mod)
```

```{r AgeDistributionMiceCheck}
ggplot(tt %>% tidyr::drop_na(Survived), aes(x=Age)) +
  geom_histogram(alpha = 0.4, binwidth = 2)+
  labs(subtitle = "Age distribution - original data")

ggplot(mice_output, aes(x=Age)) +
  geom_histogram(alpha = 0.4, binwidth = 2)+
  labs(subtitle = "Age distribution - mice data")
```

Replace the original age data with mice output
```{r AgeMiceOverride}
tt$Age <- mice_output$Age
tt$Deck <- mice_output$Deck
sum(is.na(tt$Age))
sum(is.na(tt$Deck))
```

# Feature engineering Part 2/2
## Adolescence variable
Lets create more variables having complete information and check if there is a connection.
```{r Adolescence}
tt <- tt %>% 
  mutate(Adolescence = factor(case_when(Age <= 3 ~ "Baby",
                                        Age <= 12 ~ "Child",
                                        Age <= 19 ~ "Teenager",
                                        Age <= 34 ~ "YoungAdult",
                                        Age <= 64 ~ "MiddleAged",
                                        TRUE ~ "Senior"), 
                              levels = c("Baby", "Child", "Teenager", "YoungAdult", "MiddleAged", "Senior"), 
                              ordered = T))

ggplot(tt %>% tidyr::drop_na(Survived), aes(x=Adolescence, fill = Survived)) +
  geom_bar(alpha = 0.4)+
  labs(subtitle = "Survival by Adolescence",
       x = NULL)
```

I have to admit I am happy with that category. 

## Motherhood variable
Lets see if mothers are favoured to survive.

```{r Motherhood}
tt <- tt %>% 
  mutate(Motherhood = factor(case_when(Age < 18 ~ "NotMother",
                                       Parch == 0 ~ "NotMother",
                                       Sex == "male" ~ "NotMother",
                                       Title == "Miss" ~ "NotMother",
                                       TRUE ~ "Mother")))

ggplot(tt %>% tidyr::drop_na(Survived), aes(x=Motherhood, fill = Survived)) +
  geom_bar(alpha = 0.4)+
  labs(subtitle = "Survival by Motherhood",
       x = NULL)
```
## Surname Frequency

```{r}
tt <- transform(tt, SurnameFreq = ave(seq(nrow(tt)), Surname, FUN=length))

ggplot(tt %>% tidyr::drop_na(Survived), aes(x=SurnameFreq, fill = Survived)) +
  geom_bar(alpha = 0.4)+
  labs(subtitle = "Survival by SurnameFreq",
       x = NULL)

tt <- tt %>% mutate(SurnameFreqCat = factor(case_when(SurnameFreq == 1 ~ "One",
                                               SurnameFreq <= 3 & SurnameFreq > 1 ~ "2to3",
                                               TRUE ~ "over4")))

ggplot(tt %>% tidyr::drop_na(Survived), aes(x=SurnameFreqCat, fill = Survived)) +
  geom_bar(alpha = 0.4)+
  labs(subtitle = "Survival by SurnameFreqCat",
       x = NULL)
```

## Final touches
Checking if there is missing data:
```{r fig.height=10, fig.width=27}
md.pattern(tt)
```
There is no missing data. So now is the time to split the dataset to train and test datasets.
```{r ttsubsetting}
train <- tt[1:891,]
test <- tt[892:1309,]
```

# RandomForest Model

Using randomForest classification algorithm from caret package.

```{r}
set.seed( seed = 666)

training <- train %>%
  select(Pclass,
         Sex,
         Age,
         Fare,
         Title2,
         FamilySizeCat,
         Adolescence,
         Motherhood,
         Deck,
         Survived) 

model_RF <- train(Survived~.,
                  data=training,
                  method="rf",
                  metric="Accuracy",
                  preProcess = "scale",
                  tuneGrid=expand.grid(.mtry=c(2:5)),
                  trControl=trainControl(method="repeatedcv", number=10, repeats=3),
                  ntree = 2000)

print(model_RF)
plot(model_RF)
```


## Tuning the model

After spending some time picking manually the best combinations of variables I thought that there should be a better automated way of doing this since the calculations are not so robust and searching for optimal parameters should not be done manually. Thus, I decided to do some digging. I found some YT videos and read some articles. After trying some methods I decided to use 'caret' package and implement the approach from this articles:
https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/
https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/

### Variable importance

If you decide to use something more like xgboost it is good to look for correlation matrixes and delete variables with correlation over 75% (leave only one) - its a strandard practice.
As I have features instead of gradients right now I will look on var importance to determine the best variables.

```{r}
importance <- varImp(model_RF, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
```


I decided to find the best value for mtry and then the optimal ntree value, while skipping potential interaction effects between the parameters.

### Grid search

Lets try Grid Search method to find best value for mtry (no. of variables randomly sampled as candidates at each tree split):

```{r tuningRF_mtry1}
# set.seed(seed = 7)
# # Create model with default parameters
# training <- train %>% 
#   select(Pclass, Sex, Age, SibSp, Parch, Fare, Embarked,
#          Title, FamilySizeCat, Adolescence, Motherhood, 
#          Deck, FamilySize, CabinV2, 
#          Survived)
# 
# rf_default <- train(Survived~., 
#                     data=training, 
#                     method="rf", 
#                     metric="Accuracy", 
#                     tuneGrid=expand.grid(.mtry=c(1:10)), 
#                     trControl=trainControl(method="repeatedcv", number=10, repeats=3, search="grid"))
# print(rf_default)
# plot(rf_default)
```

### Number of trees

```{r tuningRF_ntree}
# set.seed(seed = 666)
# 
# training <- train %>%
#   select(Pclass,
#          Sex,
#          Age,
#          Fare,
#          Title,
#          FamilySizeCat,
#          Adolescence,
#          Motherhood,
#          Deck,
#          Survived) 
# 
# # Manual Search
# modellist <- list()
# 
# for (ntree in c(1600, 1650, 1700, 1750, 1800, 1850, 1900, 1950, 2000)) {
# 	
#   fit <- train(Survived~.,
#                data=training,
#                method="rf",
#                metric="Accuracy",
#                tuneGrid = expand.grid(.mtry=4),
# 	             ntree = ntree)
# 	
# 	key <- toString(ntree)
# 	modellist[[key]] <- fit
# }
# 
# # compare results
# results <- resamples(modellist)
# summary(results)
# dotplot(results)
```

## Model error

```{r RFmodelError}
plot(model_RF)
model_RF$confusion
```


# Making prediction
```{r pred0}
prediction <- predict(model_RF, test)

solution <- data.frame(PassengerID = test$PassengerId, Survived = prediction)
# solution %>% str()

solution <- solution %>% 
  mutate(PassengerID = as.integer(PassengerID),
         Survived = as.integer(Survived)-1)

write.csv(solution, file = 'RF_mod_Solution.csv', row.names = F)
```

# XGBoost Model

Since xgboost needs numerical variables I need to convert the data accordingly. For that I am using library(vtreat). I saw that someone on Kaggle managed to create a nice treatment plan to do one-hot-encoding without much hassle, but on different data set. Reference:
https://www.kaggle.com/pelkoja/visual-xgboost-tuning-with-caret

```{r treatment plan}
xgbtt <- tt %>% 
  select(-Name, -Ticket, -Surname, -Survived) %>% # all vars -> xgboost acc: Accuracy/Kappa 0.7922535/0.5606442
  select(-SurnameFreq, -Title, -FamilySize, -CabinV2, -Age, -SibSp, -Parch, -SurnameFreqCat, -Cabin)

xgbtt %>% colnames()
  
treat_plan <- vtreat::designTreatmentsZ(
  dframe = xgbtt, # training data
  varlist = colnames(xgbtt) %>% .[. != "PassengerId"], # input variables = all training data columns, except id
  codeRestriction = c("clean", "isBAD", "lev"), # derived variables types (drop cat_P)
  verbose = FALSE) # suppress messages

score_frame <- treat_plan$scoreFrame %>% 
  select(varName, origName, code)

head(score_frame)
```

```{r}
# list of variables without the target variable
# xgbtt$Survived <- as.numeric(-1)
rm(xgb_treated)
xgb_treated <- vtreat::prepare(treat_plan, xgbtt)
```


```{r , height=25, fig.width=25}
library(corrplot)
cor(xgb_treated, method = c("spearman")) %>% corrplot()
```

Delete highly correlated variables
```{r}
hc <- findCorrelation(cor(xgb_treated, method = c("spearman")), cutoff=0.65)# %>% sort(.)
reduced_Data = xgb_treated[,-c(hc)]
```

```{r, height=25, fig.width=25}
cor(reduced_Data, method = c("spearman")) %>% corrplot()
```
```{r}
xgb_tr <- reduced_Data[1:891,]
xgb_te <- reduced_Data[892:1309,]
```


```{r}
input_x <- as.matrix(reduced_Data[1:891,])
input_y <- factor(train$Survived)
```

Default hyperparameters
```{r}
grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

train_control <- caret::trainControl(
  method = "cv",
  verboseIter = F, # if u want training log
  allowParallel = TRUE, # FALSE for reproducible results 
)

xgb_base <- caret::train(
  x = input_x,
  y = input_y,
  trControl = train_control,
  tuneGrid = grid_default,
  method = "xgbTree",
  verbose = TRUE
)

print(xgb_base)
```
## Grid Search for Hyperparameters

### Number of Iterations and the Learning Rate

To get reasonable running time while testing hyperparameter combinations with caret we don‚Äôt want to go over 1000. Then, we want to find a good enough learning rate for this number of trees, as for lower learning rates 1000 iterations might not be enough.

Next, as the maximum tree depth is also depending on the number of iterations and the learning rate, we want to experiment with it at this point to narrow down the possible hyperparameters. We‚Äôll also create a helper function to create the visualizations with ggplot2, called tuneplot():

```{r Number of Iterations and the Learning Rate}
# note to start nrounds from 200, as smaller learning rates result in errors so
# big with lower starting points that they'll mess the scales

nrounds <- 1100

tune_grid <- expand.grid(
  nrounds = seq(from = 200, to = nrounds, by = 20),
  eta = c(0.025, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

tune_control <- caret::trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  #index = createFolds(tr_treated$Id_clean), # fix the folds
  verboseIter = FALSE, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_tune <- caret::train(
  x = input_x,
  y = input_y,
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = TRUE
)

# helper function for the plots
tuneplot <- function(x) {
  ggplot(x) +
    coord_cartesian(ylim = c(max(x$results$Accuracy), 
                             min(x$results$Accuracy)))
}

print(xgb_tune)
tuneplot(xgb_tune)
```
```{r}
xgb_tune$bestTune
```
### Maximum Depth and Minimum Child Weight

```{r Maximum Depth and Minimum Child Weight}
tune_grid2 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 20),
  eta = xgb_tune$bestTune$eta,
  max_depth = ifelse(xgb_tune$bestTune$max_depth == 2,
    c(xgb_tune$bestTune$max_depth:4),
    xgb_tune$bestTune$max_depth - 1:xgb_tune$bestTune$max_depth + 1),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(1, 2, 3),
  subsample = 1
)

xgb_tune2 <- caret::train(
  x = input_x,
  y = input_y,
  trControl = tune_control,
  tuneGrid = tune_grid2,
  method = "xgbTree",
  verbose = TRUE
)

print(xgb_tune2)
tuneplot(xgb_tune2)
```
```{r}
xgb_tune2$bestTune
```
### Column and Row Sampling

```{r Column and Row Sampling}
tune_grid3 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 20),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = c(0.4, 0.6, 0.8, 1.0),
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = c(0.5, 0.75, 1.0)
)

xgb_tune3 <- caret::train(
  x = input_x,
  y = input_y,
  trControl = tune_control,
  tuneGrid = tune_grid3,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune3)
```
### Gamma
```{r Gamma}
tune_grid4 <- expand.grid(
  nrounds = seq(from = 50, to = nrounds, by = 20),
  eta = xgb_tune$bestTune$eta,
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = c(0, 0.05, 0.1, 0.5, 0.7, 0.9, 1.0),
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune4 <- caret::train(
  x = input_x,
  y = input_y,
  trControl = tune_control,
  tuneGrid = tune_grid4,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune4)
```
### Reducing the Learning Rate
```{r Reducing the Learning Rate}
tune_grid5 <- expand.grid(
  nrounds = seq(from = 200, to = 2500, by = 25),
  eta = c(0.01, 0.015, 0.025, 0.05, 0.1),
  max_depth = xgb_tune2$bestTune$max_depth,
  gamma = xgb_tune4$bestTune$gamma,
  colsample_bytree = xgb_tune3$bestTune$colsample_bytree,
  min_child_weight = xgb_tune2$bestTune$min_child_weight,
  subsample = xgb_tune3$bestTune$subsample
)

xgb_tune5 <- caret::train(
  x = input_x,
  y = input_y,
  trControl = tune_control,
  tuneGrid = tune_grid5,
  method = "xgbTree",
  verbose = TRUE
)

tuneplot(xgb_tune5)
```

```{r tune_atlast}
xgb_tune5$bestTune
```
## Final XGBoost model

```{r finalxgbmodel}
final_grid <- expand.grid(
  nrounds = xgb_tune5$bestTune$nrounds,
  eta = xgb_tune5$bestTune$eta,
  max_depth = xgb_tune5$bestTune$max_depth,
  gamma = xgb_tune5$bestTune$gamma,
  colsample_bytree = xgb_tune5$bestTune$colsample_bytree,
  min_child_weight = xgb_tune5$bestTune$min_child_weight,
  subsample = xgb_tune5$bestTune$subsample
)

xgb_model <- caret::train(
  x = input_x,
  y = input_y,
  trControl = train_control,
  tuneGrid = final_grid,
  method = "xgbTree",
  verbose = TRUE
)
```

```{r finalxgbaccuracy}
print(xgb_model)
```

# Making prediction
```{r pred3}
prediction <- predict(xgb_model, xgb_te)

solution <- data.frame(PassengerID = test$PassengerId, Survived = prediction)
# solution %>% str()

solution <- solution %>% 
  mutate(PassengerID = as.integer(PassengerID),
         Survived = as.integer(Survived)-1)

write.csv(solution, file = 'RF_mod_Solution.csv', row.names = F)
```